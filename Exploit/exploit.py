from pwnFramework import ExploitFramework
import langchain
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI
import os
import subprocess
import re
from prompts import *

load_dotenv()
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", temperature=0)

framework = ExploitFramework(binary_path="./vuln", source_code="source.c")
framework.perform_static_analysis()
framework.readFile("source.c")

#prompt_template.invoke({"source_code": source_code, "staticResult":staticResult})
prompt = prompt_template.format_prompt(
    source_code=framework.source_code,
    binary_path=framework.binary_path,
    static_analysis=framework.static_analysis
)

context = []
response = llm.invoke(prompt)

cmd = response.content
where_to_run = ''

print(cmd)

cmd_to_run = re.search('^\((.+)\)\s(.+)', cmd)
where_to_run = cmd_to_run.group(1)
cmd = cmd_to_run.group(2)

if where_to_run == 'shell':
    output = framework.run_command(cmd)

    prompt_output_summary = prompt_template_output_summary.format_prompt(
        last_command=cmd,
        command_output=output
    )

    output_summary_response = llm.invoke(prompt_output_summary)

    context.append(f'Command {len(context) + 1}\n{cmd}\n{output_summary_response.content}\n')
    print(context)

    prompt_subsequent = prompt_template_subsequent.format_prompt(
        source_code=framework.source_code,
        binary_path=framework.binary_path,
        static_analysis=framework.static_analysis,
        context=''.join(context)
    )

    subsequent_response = llm.invoke(prompt_subsequent)
    print(subsequent_response.content)

