from pwnFramework import ExploitFramework
import langchain
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI
import os
import subprocess
import re
from prompts import *
from pygdb import GdbShell

load_dotenv()
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", temperature=0)

framework = ExploitFramework(binary_path="vuln", source_code="source.c")
framework.perform_static_analysis()
framework.readFile("source.c")

#prompt_template.invoke({"source_code": source_code, "staticResult":staticResult})
prompt = prompt_template.format_prompt(
    source_code=framework.source_code,
    binary_path=framework.binary_path,
    static_analysis=framework.static_analysis
)

context = []
response = llm.invoke(prompt)

cmd = response.content
where_to_run = ''
print("Initial command: ", cmd)

while(True):
    if "PLEASE EXIT" in cmd:
        print(cmd)
        break

    if "gdb" in cmd:
        print("BHENCHODA WAR GYA: ",cmd)
        context.append(f'Command {len(context) + 1}\n{cmd}\n')
        gdb = GdbShell(framework.binary_path)
        gdb.start_shell()
        while(True):
            if 'close gdb' in cmd:
                context.append(f'Command {len(context) + 1}\n{cmd}\n')
                prompt_subsequent = prompt_template_subsequent.format_prompt(
                source_code=framework.source_code,
                binary_path=framework.binary_path,
                static_analysis=framework.static_analysis,
                context=''.join(context))
                gdb.stop_shell()
                subsequent_response = llm.invoke(prompt_subsequent)
                cmd = subsequent_response.content
                break

            print("Recieved command: ", cmd)
            prompt_subsequent = prompt_template_subsequent.format_prompt(
                source_code=framework.source_code,
                binary_path=framework.binary_path,
                static_analysis=framework.static_analysis,
                context=''.join(context))

            subsequent_response = llm.invoke(prompt_subsequent)
            cmd = subsequent_response.content
            output = gdb.execute_command(cmd)
            print(output)
            prompt_output_summary = prompt_template_output_summary.format_prompt(
            last_command=cmd,
            command_output=output)    
            output_summary_response = llm.invoke(prompt_output_summary)
            context.append(f'Command {len(context) + 1}\n{cmd}\n{output_summary_response.content}\n')

    else:
        cmd_to_run = re.search('^\((.+)\)\s(.+)', cmd)
        print("Non gdb command: ", cmd)
        where_to_run = cmd_to_run.group(1)
        cmd = cmd_to_run.group(2) 
        if where_to_run == 'shell':
            output = framework.run_command(cmd)

            prompt_output_summary = prompt_template_output_summary.format_prompt(
                last_command=cmd,
                command_output=output
            )

            output_summary_response = llm.invoke(prompt_output_summary)

            context.append(f'Command {len(context) + 1}\n{cmd}\n{output_summary_response.content}\n')
            print(context)

            prompt_subsequent = prompt_template_subsequent.format_prompt(
                source_code=framework.source_code,
                binary_path=framework.binary_path,
                static_analysis=framework.static_analysis,
                context=''.join(context)
            )

            subsequent_response = llm.invoke(prompt_subsequent)
            print(subsequent_response.content)
            cmd = subsequent_response.content

