from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAI
from prompts import *
import os

load_dotenv()
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o", temperature=0)


def get_exploit_path(source_code, static_analysis):
    prompt = prompt_exploit_path.format_prompt(
        source_code=source_code,
        static_analysis=static_analysis
    )
    response = llm.invoke(prompt)
    return response.content

def get_first_command(exploit_path, source_code, binary_path, static_analysis):
    prompt = prompt_template.format_prompt(
        exploit_path=exploit_path,
        source_code=source_code,
        binary_path=binary_path,
        static_analysis=static_analysis
    )
    response = llm.invoke(prompt)
    return response.content

def get_next_command(exploit_path, source_code, binary_path, static_analysis, context):
    prompt = prompt_template_subsequent.format_prompt(
        exploit_path=exploit_path,
        source_code=source_code,
        binary_path=binary_path,
        static_analysis=static_analysis,
        context=''.join(context)
    )
    response = llm.invoke(prompt)
    return response.content

def get_output_summary(cmd, output):
    prompt = prompt_template_output_summary.format_prompt(
        last_command=cmd,
        command_output=output
    )
    response = llm.invoke(prompt)
    return response.content

def final_script(exploit_path, source_code, binary_path, static_analysis, context):
    prompt = prompt_template_final_script.format_prompt(
        exploit_path=exploit_path,
        source_code=source_code,
        binary_path=binary_path,
        static_analysis=static_analysis,
        context=''.join(context)
    )
    response = llm.invoke(prompt)
    return response.content