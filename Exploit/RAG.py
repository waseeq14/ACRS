import os
from markdown import markdown
from bs4 import BeautifulSoup
from langchain.vectorstores import FAISS
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain.chains import ConversationalRetrievalChain
import subprocess

# Function to process markdown files and convert them into plain text
def load_md_files_recursive(directory_path):
    documents = []
    for root, _, files in os.walk(directory_path):
        for file_name in files:
            if file_name.endswith(".md"):
                file_path = os.path.join(root, file_name)
                with open(file_path, 'r', encoding='utf-8') as file:
                    md_content = file.read()
                html = markdown(md_content)
                plain_text = BeautifulSoup(html, 'html.parser').get_text()
                documents.append(plain_text)
    return documents

# Function to process a source.c file
def process_source_file(source_file_path):
    with open(source_file_path, 'r', encoding='utf-8') as file:
        source_code = file.read()
    return source_code

# Function to analyze a binary file
def analyze_binary(binary):
    try:
        output = objdump_output = subprocess.run(
            ["objdump", "-d", binary], capture_output=True, text=True
        ).stdout
        return output
    except Exception as e:
        return f"Error analyzing binary file: {str(e)}"

# Define the directory containing .md files
md_directory = "RAG_data"

# Load and prepare the documents
print("Loading markdown files...")
documents = load_md_files_recursive(md_directory)

# Split the documents into manageable chunks
print("Splitting documents into chunks...")
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = text_splitter.create_documents(documents)
print(split_docs)

# Embed the documents and save the FAISS index
print("Embedding documents...")
embeddings = OpenAIEmbeddings()
faiss_index = FAISS.from_documents(split_docs, embeddings)
faiss_index.save_local("knowledge_base_faiss")

# Load the FAISS index for retrieval
print("Loading FAISS index...")
loaded_vectors = FAISS.load_local("knowledge_base_faiss", embeddings, allow_dangerous_deserialization=True)

# Initialize the LLM and RAG chain
print("Initializing RAG pipeline...")
llm = ChatOpenAI(model="gpt-4o")
QA = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=loaded_vectors.as_retriever()
)

# Predefined prompt
predefined_prompt = "AYou are an Expert CTF player, tell me how to exploit "

# Function to interact with the RAG pipeline
chat_history = []

def rag_with_files(source_file, binary_file):
    
    
    # Combine input with predefined prompt
    combined_prompt = (
        f"{predefined_prompt}\n\n"
        #f"### Source Code:\n{source_code}\n\n"
        #f"### Binary Analysis:\n{binary_analysis}\n"
    )
    
    response = QA({"question": combined_prompt, "chat_history": chat_history})
    chat_history.append((combined_prompt, response['answer']))
    return response['answer'].strip()

# Example Usage
if __name__ == "__main__":
    print("RAG pipeline is ready!")
    source_file = input("Enter the path to the source.c file: ")
    binary_file = input("Enter the path to the binary file: ")
    
    answer = rag_with_files(source_file, binary_file)
    print("\nAnalysis Result:\n", answer)
