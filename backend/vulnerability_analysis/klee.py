import os
import subprocess
import json
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from dotenv import load_dotenv
from langchain.schema.runnable import RunnablePassthrough, RunnableSequence
import re
from langchain.chat_models import ChatOpenAI
import warnings

# Suppress all warnings
warnings.filterwarnings("ignore")


def color_text(text, color):
    """Helper function to add ANSI color codes to text."""
    colors = {
        "red": "\033[91m",
        "green": "\033[92m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "cyan": "\033[96m",
        "reset": "\033[0m",
        "white": "\033[97m"
    }
    return f"{colors[color]}{text}{colors['reset']}"


def fix_klee_includes(code):
    lines = code.splitlines()
    processed_lines = []

    for line in lines:
        if "#include #include" in line:
            print(line)
            line = line.replace("#include #include", "#include")
        processed_lines.append(line)

    return "\n".join(processed_lines)


class KleeProcessor:
    def __init__(self, source_code_path, llm):
        self.source_code_path = source_code_path
        self.llm = llm
        self.klee_friendly_path = None
        self.bc_file = None
        self.klee_output_dir = "vulnerability_analysis/klee-last"

    def preprocess_code_with_llm(self):
        with open(self.source_code_path, "r") as file:
            source_code = file.read()

        prompt = PromptTemplate(
            template=(
                "DONT GO AGAINST THE INSTRUCTIONS"
                "You are tasked with making the following C/C++ code compatible with KLEE for symbolic execution. If there is no main() function,define one."
                "Without wrapping it in triple backticks or any formatting. Just output the plain code."
                "The goal is to identify vulnerabilities such as buffer overflows, use-after-free, integer overflows or other memory-related issues. "
                "Replace any user input mechanisms (e.g., scanf, fgets) or external data sources with symbolic variables using KLEE's APIs. "
                "Remove all printf statements which include symbolic variables or any variable which is their result"
                "Directly printing pointers like buffer is invalid in klee_print_expr. Instead, access specific bytes (e.g., buffer[0])"
                "Ensure the code includes <klee/klee.h>, and use klee_make_symbolic() to replace user inputs or dynamic data sources. "
                "klee_print_expr() does not handle format strings like printf and requires the symbolic expression to be passed directly without formatting. For example:- Using printf: printf (\"Buffer content: %\s\n\", buffer);- The equivalent klee_print_expr: klee_print_expr(\"Buffer content:\", buffer);"
                "Avoid marking local variables as symbolic if they are directly influenced by symbolic inputs, as this is unnecessary. "
                "KLEE will automatically propagate symbolic properties through operations. "
                "The modified code should strictly adhere to KLEE-compatible practices. Proof-check the modified code "
                "for correctness and proper integration of KLEE-specific modifications. Do not include comments in the code.\n\n"
                "Original Code:\n{source_code}\n\n"
                "KLEE-Compatible Code:"
            ),
            input_variables=["source_code"]
        )

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        klee_friendly_code = llm_chain.run({"source_code": source_code})
        klee_friendly_code = fix_klee_includes(klee_friendly_code)

        self.klee_friendly_path = self.source_code_path.replace(".c", "_klee.c")
        with open(self.klee_friendly_path, "w") as file:
            file.write(klee_friendly_code)
        print(f"KLEE-compatible code written to {self.klee_friendly_path}")

        return klee_friendly_code

    def generate_llvm_ir(self, output_name="output"):
        self.bc_file = f"{output_name}.bc"

        clang_command = f"clang-13 -emit-llvm -I /snap/klee/10/usr/local/include -g -fsanitize=signed-integer-overflow -fsanitize=undefined -fsanitize=signed-integer-overflow -c -o vulnerability_analysis/{self.bc_file} {self.klee_friendly_path}"
        subprocess.run(clang_command, shell=True, check=True)

        print(f"LLVM bitcode generated: {self.bc_file}")

    def run_klee(self):
        klee_command = f"klee --libc=uclibc --posix-runtime /home/parrot/Desktop/fyp/backend/vulnerability_analysis/{self.bc_file}"

        print(f"Running KLEE with command: {klee_command}")
        subprocess.run(klee_command, shell=True, check=True)
        print(f"KLEE execution completed for {self.bc_file}")

    def parse_klee_output(self):
        test_cases = []
        klee_messages = ""

        error_files = [file for file in os.listdir(self.klee_output_dir) if file.endswith(".err")]

        for err_file in error_files:
            test_case_base = err_file.split(".")[0]

            ktest_file = os.path.join(self.klee_output_dir, f"{test_case_base}.ktest")
            err_file_path = os.path.join(self.klee_output_dir, err_file)

            if os.path.exists(ktest_file):
                try:
                    ktest_output = subprocess.check_output(
                        f"ktest-tool {ktest_file}", shell=True, text=True
                    )

                    with open(err_file_path, "r") as f:
                        err_content = f.read()

                    test_cases.append({
                        "ktest_file": ktest_file,
                        "err_file": err_file_path,
                        "ktest_output": ktest_output,
                        "err_content": err_content
                    })
                except subprocess.CalledProcessError as e:
                    print(f"Error reading {ktest_file} or {err_file}: {e}")

        prompt = PromptTemplate(
            template=(
                "You are analyzing KLEE error reports to identify vulnerabilities. "
                "For each error, provide a concise summary of the vulnerability, the test case name, and the input that caused it (check the input from ktest output given to you). "
                "For example: Vulnerability: \nTest Case Name: \nInput that triggered it: \n"
                "Do not write unnecessary details."
                "\n\n"
                "Test Cases and Errors:\n{test_cases}"
            ),
            input_variables=["test_cases"]
        )

        formatted_test_cases = [
            f"Test Case: {case['ktest_file']}\nInput: {case['ktest_output']}\nError: {case['err_content']}"
            for case in test_cases
        ]

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)

        try:
            summary = llm_chain.run({
                "test_cases": "\n\n".join(formatted_test_cases)
            })
            print(color_text("\nAnalysis:\n", "yellow"), color_text(summary, "white"))
            with open('klee_output.txt', 'w') as llm_analysis_output:
                llm_analysis_output.write(summary)
            return summary
        except Exception as e:
            print(f"Error during LLM analysis: {e}")