from langchain import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import OpenAI
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
import os

class PatchGenerator:
    def __init__(self, source_code, analysis):
        self.source_code = source_code
        self.analysis = analysis

    def generate_patched_code(self):
        # with open(self.source_code_path, "r") as code_file:
        #     vulnerable_code = code_file.read()

        # with open(self.analysis_file, "r") as analysis_file:
        #     vulnerability_analysis = analysis_file.read()

        prompt = PromptTemplate(
            template=(
                "You are a cybersecurity expert and C/C++ developer. Your task is to patch the vulnerable code given below using the provided vulnerability analysis. "
                "Ensure to strictly follow secure coding practices, and apply only necessary changes to eliminate the vulnerabilities. "
                "The patched code must be fully functional and written in standard C/C++. "
                "Add comments on the lines you modify, explaining the fix (e.g., // Fixed buffer overflow). "
                "Do not wrap the code in triple backticks or any markdown formatting, just return the patched code as plain text.\n\n"
                "Vulnerability Analysis:\n{vulnerability_analysis}\n\n"
                "Vulnerable Code:\n{vulnerable_code}\n\n"
                "Patched Code with comments:"
            ),
            input_variables=["vulnerable_code", "vulnerability_analysis"]
        )

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        patched_code = llm_chain.run({
            "vulnerable_code": self.source_code,
            "vulnerability_analysis": self.analysis
        })

        # self.patched_code_path = self.source_code_path.replace(".c", "_patched.c")
        # with open(self.patched_code_path, "w") as patched_file:
        #     patched_file.write(patched_code)

        # print(f"Patched code written to {self.patched_code_path}")
        return patched_code

    def generate_exploit_path(self):
        # with open(self.source_code_path, "r") as code_file:
        #     vulnerable_code = code_file.read()

        # with open(self.analysis_file, "r") as analysis_file:
        #     vulnerability_analysis = analysis_file.read()

        prompt = PromptTemplate(
            template=(
                "You are a security analyst tasked with identifying how an attacker might exploit the following code vulnerabilities. "
                "Given the vulnerability analysis and the vulnerable source code, generate an exploit path. "
                "The output should include:\n"
                "- Root cause of the vulnerability\n"
                "- Step-by-step method an attacker could follow to exploit it\n"
                "- Possible consequences (e.g., privilege escalation, remote code execution, information leak)\n"
                "- Assumptions required for the attack (e.g., attacker controls input, code runs as root, etc.)\n\n"
                "Don't include any actual exploit code, just a well-reasoned explanation of the exploitation path.\n\n"
                "Vulnerability Analysis:\n{vulnerability_analysis}\n\n"
                "Vulnerable Code:\n{vulnerable_code}\n\n"
                "Exploit Path:"
            ),
            input_variables=["vulnerable_code", "vulnerability_analysis"]
        )

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        exploit_path = llm_chain.run({
            "vulnerable_code": self.source_code,
            "vulnerability_analysis": self.analysis
        })
        # exploit_path_file = self.source_code_path.replace(".c", "_exploit_path.txt")
        # with open(exploit_path_file, "w") as file:
        #     file.write(exploit_path)
        print("\n=== Exploit Path ===\n")
        print(exploit_path)
        return exploit_path

    def setupEnv(self):
        load_dotenv()
        OpenAI.api_key = os.getenv("OPENAI_API_KEY")
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)

    def patch_descriptor(self, generated_patch):
        prompt = PromptTemplate(
            template=(
                "You are a code analysis assistant. You will be given two code snippets:\n"
                "1. The **original (vulnerable)** source code.\n"
                "2. The **patched** version of the code.\n\n"
                "Your task is to:\n"
                "- Describe the changes made in the patched code.\n"
                "- Explain what issue or vulnerability the patch addresses.\n"
                "- Clarify how the patch mitigates the issue.\n\n"
                "**Original Code:**\n"
                "```c\n{source_code}\n```\n\n"
                "**Patched Code:**\n"
                "```c\n{generated_patch}\n```\n\n"
                "You must follow this output format: \n"
                "**Changes Made: **\n"
                "**Patch Description: **\n\n"
                
            ),
            input_variables=["source_code", "generated_patch"]
        )

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        patch_description = llm_chain.run({
            "source_code": self.source_code,
            "generated_patch": generated_patch
        })
        return patch_description


# if __name__ == "__main__":
#     patch = PatchGenerator("/home/parrot/Desktop/fyp/projects/f90db8a2-245f-48a9-8e39-c96bbb43aaf4/code.c","/home/parrot/Desktop/fyp/projects/f90db8a2-245f-48a9-8e39-c96bbb43aaf4/klee_output.txt",llm)
#     patchedCode = patch.generate_patched_code()
#     print(patchedCode)

#     exploitPath = patch.generate_exploit_path()
#     print(exploitPath)