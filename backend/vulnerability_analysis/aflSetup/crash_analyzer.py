import os
import subprocess
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

class CrashAnalyzer:
    def __init__(self, binary_path, crashes_dir, llm):
        self.binary_path = binary_path
        self.crashes_dir = crashes_dir
        self.llm = llm

    def analyze_crashes(self):
        """Runs each crash input on the binary, captures output, and analyzes with LLM."""
        print("[+] Analyzing AFL++ crashes...")
        analysisOutput = []

        if not os.path.exists(self.crashes_dir):
            print(f"[-] Crash directory `{self.crashes_dir}` not found!")
            return

        crash_files = [f for f in os.listdir(self.crashes_dir) if f.startswith("id:")]
        if not crash_files:
            print("[-] No crash files found.")
            return

        for crash_file in crash_files:
            crash_path = os.path.join(self.crashes_dir, crash_file)

            print(f"\n[*] Testing crash: {crash_file}")
            try:
                result = subprocess.run(
                    [self.binary_path, crash_path],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=5  # Prevent hangs
                )

                crash_output = result.stderr + result.stdout

                # Use LLM to analyze the crash
                analysis = self.analyze_with_llm(crash_output)

                analysisOutput.append(f"[*] LLM Analysis for {crash_file}: {analysis}\n")
                print(analysisOutput)

            except subprocess.TimeoutExpired:
                print(f"[-] Crash {crash_file} caused a timeout.")
        
        return analysisOutput
                

    def analyze_with_llm(self, crash_output):
        """Passes crash output to LLM for vulnerability classification."""
        prompt = PromptTemplate(
            template=(
                "Analyze the following program crash and determine the likely cause of the vulnerability. "
                "Classify it into categories.\n\n"
                "Crash Output:\n{crash_output}\n\n"
                "KEEP YOUR ANALYSIS CONCISE"
            ),
            input_variables=["crash_output"]
        )

        llm_chain = LLMChain(llm=self.llm, prompt=prompt)
        return llm_chain.run({"crash_output": crash_output})



